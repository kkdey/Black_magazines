---
title: 'Aggregating texts and pre-processing for word2vec: Ebony and Black World/Negro
  Digest'
author: "Kushal K Dey"
date: "7/27/2018"
output: html_document
---

## Intro

In this first tutorial chapter, we explain how to aggregate the yearly text data for each magazine 
of interest - Black World/Negro Digest and Ebony - and then train a word2vec model on the aggregated text data.

*Why yearly analysis*? 

We carry out our analysis at the yearly level, because the monthly text data are more noisy
and also contains very few occurrences of most words to accurately train the word2vec model.

## Packages

```{r message=FALSE, warning=FALSE}
library(wordVectors)
```

## Black World/Negro Digest

We have saved the Black World/Negro Digest magazines in the folder `BL_ND` in the 
Github repo `Black_Magazines` with a designated folder for each year - `BL_ND_1961` 
subfolder inside `BL_ND` consists of raw magazine text data for each month of 1961.
We aggregate these monthly text data into one single text data file called `pooled.txt`
and then run word2vec on this aggregated text to generate the model output 
`pooled_word2vec.bin`.

WARNING : The chunk below would take approximately 1 hour to run. You can submit
them as parallel batch jobs to quicken the process.

```{r echo=TRUE, eval=FALSE}
dirs <- c("../BL_ND/BL_ND_1961/",
          "../BL_ND/BL_ND_1962/",
          "../BL_ND/BL_ND_1963/",
          "../BL_ND/BL_ND_1964/",
          "../BL_ND/BL_ND_1965/",
          "../BL_ND/BL_ND_1966/",
          "../BL_ND/BL_ND_1967/",
          "../BL_ND/BL_ND_1968/",
          "../BL_ND/BL_ND_1969/",
          "../BL_ND/BL_ND_1970/",
          "../BL_ND/BL_ND_1971/",
          "../BL_ND/BL_ND_1972/",
          "../BL_ND/BL_ND_1973/",
          "../BL_ND/BL_ND_1974/",
          "../BL_ND/BL_ND_1975/",
          "../BL_ND/BL_ND_1976/")


for(m in 1:length(dirs)){
  
  ############  Aggregate monthly texts into yearly text data #####################
  
  out <- prep_word2vec(origin = dirs[m], 
                       destination = paste0(dirs[m], "pooled.txt"), 
                       lowercase = T)
  
  ############  Train word2vec model on the yearly text data  #####################
  
  model = train_word2vec(paste0(dirs[m], "pooled.txt"),
                         paste0(dirs[m], "pooled_word2vec.bin"),
                         vectors=100,threads=10,window=10,
                         iter=50, min_count = 3, negative_samples=0,
                         force = TRUE)
}
```

## Ebony

We have saved the Ebony magazines in the folder `Ebony` in the 
Github repo `Black_Magazines` with a designated folder for each year - `1961` 
subfolder inside `Ebony` consists of raw magazine text data for each month of 1961.
We aggregate these monthly text data into one single text data file called `pooled.txt`
and then run word2vec on this aggregated text to generate the model output 
`pooled_word2vec.bin`.

WARNING : The chunk below would take approximately 1 hour to run. You can submit
them as parallel batch jobs to quicken the process.

```{r echo=TRUE, eval=FALSE}
dirs <- c("../Ebony/1961/", 
          "../Ebony/1962/",
          "../Ebony/1963/",
          "../Ebony/1964/",
          "../Ebony/1965/", 
          "../Ebony/1966/",
          "../Ebony/1967/", 
          "../Ebony/1968/",
          "../Ebony/1969/",
          "../Ebony/1970/", 
          "../Ebony/1971/", 
          "../Ebony/1972/",
          "../Ebony/1973/",
          "../Ebony/1974/", 
          "../Ebony/1975/",
          "../Ebony/1976/")


for(m in 1:length(dirs)){
  
  ############  Aggregate monthly texts into yearly text data #####################
  
  out <- prep_word2vec(origin = dirs[m], 
                       destination = paste0(dirs[m], "pooled.txt"), 
                       lowercase = T)
  
  ############  Train word2vec model on the yearly text data  #####################
  
  model = train_word2vec(paste0(dirs[m], "pooled.txt"),
                         paste0(dirs[m], "pooled_word2vec.bin"),
                         vectors=100,threads=10,window=10,
                         iter=50, min_count = 3, negative_samples=0,
                         force = TRUE)
}
```

## SessionInfo 

```{r}
sessionInfo()
```

